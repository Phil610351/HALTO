\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{multirow}
\begin{document}

Deep Q-Learning based Joint Dynamic Network Slicing and Resource Allocation in Multi-Tenant Edge Computing System

Network slice parameter
$\alpha, \beta$

Task format:

$J_{i}=(\rho_{i}, a_{i}, d_{i}, T_{i}^{*})$

Local execution time:
\begin{equation}
T_{i}^{l}=\frac{(1-x_{i})d_{i}}{F_{i}}
\end{equation}

Data rate:
\begin{equation}
r_{i}=b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } ) }
\end{equation}

Transmission time:
\begin{equation}
T^{tx}_{i}=\frac{x_{i}a_{i}}{r_{i}}
\end{equation}

Execution time:
\begin{equation}
T^{ex}_{i}=\frac{x_{i}d_{i}}{f_{i}}
\end{equation}

Remote execution time:
\begin{equation}
T^{o}_{i}=T^{tx}_{i}+T^{ex}_{i}
\end{equation}

Utility:
\begin{equation}
U_{i}(x,b,f)=\left\{
	\begin{array}{ll}   
	\Psi \quad \mbox{if $ T_{i} > T_{i}^{*}$}, \\
    \max(T^{o}_{i},T^{l}_{i}) \qquad \mbox{otherwise.}
	\end{array}
\right.
\end{equation}

Revenue:
\begin{equation}
Rev_{z}=\sum_{i}\rho_{i}U_{i}
\end{equation}

Cost:
\begin{equation}
C_{z}=\zeta\alpha +\xi\beta
\end{equation}

Problem formulation:
\begin{align}\
& \max \frac{1}{n}\sum_{z=0}^{n} Rev_{z}-Cost_{z}  \nonumber\\
\rm{s.t.} \quad &C1: 1 \nonumber\\ 
\quad &C2: 2 \nonumber\\
\end{align}

SP1:
\begin{align}\
& \max_{\alpha, \beta} E[ Rev_{z}(\alpha, \beta) - Cost_{z}(\alpha, \beta) ] \nonumber\\
\rm{s.t.} \quad &C1: \alpha \in [0,1] \nonumber\\ 
\quad &C2: \beta \in [0,1] \nonumber\\
\end{align}

RL:

State:

current slice scale, traffic history in last k time slots

Action:

incrase/ decrease slice scale

Reward:

\begin{equation}
r_{s,a} = \left\{
	\begin{array}{ll}   
	\Phi, \quad \mbox{if $ \sum_{n \in F} \sum_{} f_{n,w} > F$}, \\
    \sum_{i=0}^{n} \varphi C_{i} (x_{i}, f_{i}, w_{i}, e_{i}) \qquad \mbox{otherwise.}
	\end{array}
\right.
\end{equation}

\begin{equation}
V^{\pi}=E[ \Sigma_{i=0}^{\inf} \gamma r_{i} \vert s_{0}=s ]
\end{equation}

\begin{equation}
\pi^{*}=arg \max_{\pi}  Q(s,a), \forall s \in S
\end{equation}

\begin{equation}
Q^{*}(s,a)=Q(s,a)+\alpha( r + \beta \max Q(s,a) - Q(s,a))
\end{equation}

\begin{equation}
Loss(\theta)=E[ y - Q(s,a,\theta)^{2}]
\end{equation}

SP2:
\begin{align}\
& \max_{x,b,f} \sum_{i}\rho_{i}U_{i}(x_{i},b_{i},f_{i})  \nonumber\\
\rm{s.t.} \quad &C1: x_{i} \in [0,1] \nonumber\\ 
\quad &C2: b_{i} \in [0,B] \nonumber\\
\quad &C3: f_{i} \in [0,F] \nonumber\\
\quad &C4: \sum_{i}b_{i}=B \nonumber\\
\quad &C5: \sum_{i}f_{i}=F \nonumber\\
\end{align}

Theorem 1:
When minimum occur, 
\begin{equation}
x_{i}=\frac{d_{i}}{ F_{i}(\frac{a_{i}}{b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )}} + \frac{d_{i}}{f_{i}}+\frac{d_{i}}{F_{i}}) }
\end{equation}

Proof:
For $i$
\begin{equation}
U_{i}=max(T_{i}^{o}, T_{i}^{l})=max(x_{i}(\frac{a_{i}}{b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )}} + \frac{d_{i}}{f_{i}}), \frac{d_{i}}{F_{i}})
\end{equation}
\begin{equation}
T_{i}^{o}=T_{i}^{l}, x_{i}(\frac{ a_{i} }{ b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} } + \frac{d_{i}}{f_{i}})= \frac{ (1-x_{i})d_{i} }{F_{i}}
\end{equation}
After rearange equation (), we can obtain $x_{i}=\frac{d_{i}}{ F_{i}(\frac{a_{i}}{b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )}} + \frac{d_{i}}{f_{i}}+\frac{d_{i}}{F_{i}}) }$.

Reformulate:

relax $U_{i}$ to $\hat{U_{i}}=...$ no penalty

\begin{align}\
& \max_{x,b,f} \sum_{i}\rho_{i}x_{i}(\frac{a_{i}}{b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }+\frac{d_{i}}{f_{i}})  \nonumber\\
\rm{s.t.} \quad &C2-C5 \nonumber\\
\end{align}
Proposition 2:
Objective is convex

Proof:
The Hessian matrix of (12) consists of, where. Thus, the Hessian matrix of (12) is a positive definite matrix, and hence is convex.
\begin{equation}
\frac{\partial^{2}}{\partial^{2}b^{2}_{i}}= ... >=0
\end{equation}

\begin{equation}
\frac{\partial^{2}}{\partial^{2}f^{2}_{i}}= ... >=0
\end{equation}

Theorem 2:
Given a set of offloading request, the optimal bandwidth and computing resource allocation is
\begin{equation}
b_{i}=\frac{\sqrt{\frac{\rho_{i}x_{i}a_{i}}{\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }}}{\sum_{i}\sqrt{\frac{\rho_{i}x_{i}a_{i}}{\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }}}B, f_{i}=\frac{ \sqrt{ \rho_{i}x_{i}d_{i}} } { \sum_{i}\sqrt{\rho_{i}x_{i}d_{i}} }F
\end{equation}

Proof:
We introduce Lagrange multiplier to solve the problem (12). The Lagrange dual function of (12) is
\begin{equation}
L=\sum_{i}\rho_{i}x_{i}(\frac{a_{i}}{b_{i}\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }+\frac{d_{i}}{f_{i}}) + \lambda(\sum_{i}b_{i}-B) + \mu(\sum_{i}f_{i}-F)
\end{equation}

\begin{equation}
	\left\{
	\begin{array}{ll}   
	\frac{\partial L}{\partial b_{i}} =  + \lambda = 0, \forall i \\
    \frac{\partial L}{\partial \lambda} = \sum_{i}b_{i} - B = 0
	\end{array}
\right.
\end{equation}

\begin{equation}
	\left\{
	\begin{array}{ll}   
	\frac{\partial L}{\partial f_{i}} = -\frac{\rho_{i}x_{i}d_{i}}{f_{i}^{2}} + \mu = 0, \forall i \\
    \frac{\partial L}{\partial \mu} = \sum_{i}f_{i} - F = 0
	\end{array}
\right.
\end{equation}
After solve equation (), we can obtain $b_{i}=\frac{\sqrt{\frac{\rho_{i}x_{i}a_{i}}{\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }}}{\sum_{i}\sqrt{\frac{\rho_{i}x_{i}a_{i}}{\log_{2}{ (1+\frac{g_{i}p_{i}}{ \sigma^{2} } )} }}}B, f_{i}=\frac{ \sqrt{ \rho_{i}x_{i}d_{i}} } { \sum_{i}\sqrt{\rho_{i}x_{i}d_{i}} }F$.

Alg:

init:$x_{i}= min(1,\frac{T^{*}_{i}F_{i}^{l}}{d_{i}}), \forall i$

\begin{algorithm}[t]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Initialization:}}
  \small
  \caption{QoE-Driven Iterative Video Adaptation}
  \label{alg:intra_place}
  \begin{algorithmic}[1]
  	\REQUIRE The set of
  	\ENSURE replay memory $D=\emptyset$, policy network $Q$, target network $Q$
  	\FOR{episode = 1, M}
  		\STATE Initialize state
  		\WHILE{current state is not a terminal state}
  			\STATE Randomly select an action with probability $\varepsilon$
  			\STATE Otherwise, select the action $a=maxQ$ according to policy network Q
  			\STATE Execute the action observe reward $r$
  			\STATE Store transition $(s_{i},s_{i+1}, a, r)$ in D
  			\STATE Sample random a minibatch of transitions from D
  			\STATE $y=$
  			\STATE Update weights $\theta$ of policy network Q via a learning step with optimizer
  			\STATE Reset target network Q $\leftarrow$Q every C steps
  		\ENDWHILE
  	\ENDFOR
  	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\ENSURE The set of
  \end{algorithmic}
\end{algorithm}

\end{document}